# -*- coding: utf-8 -*-
"""plant-seedlings.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gmZEXH_lFC4w8AllAmnWiwklKt0nsnNc

# Plant Seedlings Classification Project with TensorFlow 2.0

In this project, I'll work on Kaggle's Plant Seedling Classification dataset. This is a multi-class image classification project: We'd like to identify the species of a plant based on a given photograph of it. 

For the training data, we have 12 different plant species each stored in a different folder. The total number of images for training is 4750. 

We'll use TensorFlow 2.0 and Transfer Learning for this project.

https://www.kaggle.com/competitions/plant-seedlings-classification/overview/description

### Getting the data ready 

We have 12 seperate folders containing a different plant species. First, we'd like to create a Pandas dataframe so that contains the image id's together with the corresponding plant species.  

* ID - SPECIES = This is the dataframe structure we're looking for.
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os 
import tensorflow as tf

from google.colab import drive
drive.mount('/content/drive')

data_path = '/content/drive/MyDrive/PlantSeedlings/train'
image_paths = []
species = []

for spec in os.listdir(data_path):
    spec_path = os.path.join(data_path, spec)
    for img in os.listdir(spec_path):
        img_path = os.path.join(spec_path, img)
        image_paths.append(img_path)
        species.append(spec)

len(image_paths), len(species)

print(image_paths[:5])
print(species[:5])

df = pd.DataFrame({"image_path":image_paths,
                   "species":species})
df.head()

df.species.value_counts()

df.describe()

df.species.value_counts().plot.bar(figsize=(10,5));

img = plt.imread(df.image_path[100])
plt.imshow(img)
plt.show();

"""Now, we've a dataframe that consists of image filepaths with the corressponding labels. To progress further, we need to represent unique plant species with boolean values in the form of a numpy array."""

unique_species = np.unique(df['species'])
type(unique_species)

# This is an example of how we're trying to represent the species
df.species[0] == unique_species

# Let's turn every species's name into a boolean array
boolean_specs = [tf.equal(x, unique_species) for x in df['species']]
boolean_specs[:2]

print(len(boolean_specs))

"""### Creating our validation set

The first test place for our predictions must be our own validation dataset, not Kaggle's test folder.

**Problem:** We have a very orderly data frame. Since each different species are stored in a seperate folder, while looping over the folders, we ended up with a very orderly dataframe. It is healthy for our validation dataset.

First, we need to create a shuffled dataset and then re-create our boolean unique species list based on it. Only after this, we can split our data to training and validation sets.*italicized text*
"""

from sklearn.utils import shuffle

df_shuffled = shuffle(df)

df_shuffled.shape, df.shape

df_shuffled.tail(5)

unique_species

# Recreating boolean_specs with the new shuffled dataframe
boolean_specs = [x == np.array(unique_species) for x in df_shuffled['species']]

len(boolean_specs)

# Let's check if we got it right
print(df_shuffled.species.iloc[9])
index = boolean_specs[9].argmax()
print(unique_species[index])

"""Now, we got it right! We can progress and split our data into training and validation datasets."""

df_shuffled = df_shuffled.reset_index(drop=True)
df_shuffled.head()

df_shuffled.image_path[0]

X = df_shuffled.image_path
y = boolean_specs

# First we'll experiment with a 1000 images
NUM_IMAGES = 1500 #@param {type:"slider", min:1000, max:4750, step:500}

from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X[:NUM_IMAGES],
                                                  y[:NUM_IMAGES],
                                                  test_size=0.2,
                                                  random_state=42)
len(X_train), len(y_train), len(X_val), len(y_val)

X_train = X_train.reset_index(drop=True)
X_val = X_val.reset_index(drop=True)

"""### Preprocessing data
Our data is in the form of image_paths, boolean_labels. But to go further we need to create Tensors out of images that are located at the specified paths. By storing information in tensors, we can run deep learning algorithms on them by taking the advantage of GPU's.

First, we'll define a function:
1. Takes image path as an input.
2. Uses TensorFlow to read the path and save it to variable `image`.
3. Turns `image`(png file) into Tensors.
4. Resizes `image` to the shape of (224, 224).
5. Returns the modified image.
"""

import tensorflow as tf

# This is the shape of our image 
from matplotlib.pyplot import imread
image = imread(df_shuffled.image_path[90])
image.shape

# You can easily convert an image into tensors in this way:
# Interesting: The resulting tensors are the normalized (in range 0-1)
tf.constant(image)[:2]

# The function specified above
# First set the image size
IMG_SIZE = 224

def process_image(image_path):

    # Read the image path and save it to a variable
    image = tf.io.read_file(image_path)
    # Turn the image (png file) into a tensor with 3 color channels
    image = tf.image.decode_png(image, channels=3)
    # Convert the colour channels values from 0-255
    image = tf.image.convert_image_dtype(image, tf.float32)
    # Resize the image (224, 224)
    image = tf.image.resize(image, size=[IMG_SIZE, IMG_SIZE])
    return image

"""### Creating Data Batches

Instead of training the deep learning algorithm over the entire dataset, we create data batches, small samples of data, so that the algorithm can work on a single sample at a time. This is a more GPU-friendly way of training the algorithm on the data. 

TensorFlow favors the structure of `image,label` when creating data batches. 
"""

# Create a function that returns a (image,label) tuple
def get_image_label(image_path, label):

    image = process_image(image_path)

    return image, label

"""The function above gets an image path and returns a tuple that consists of processed image and the label. Let's create another function that creates data batches from given datasets, and prepares data batches while taking into account whether we're dealing with training, validation or test data. """

# Define the batch size
BATCH_SIZE = 32

def create_data_batches(x, y=None, batch_size=BATCH_SIZE, val_data=False, test_data=False):

    # We won't have labels if we have a test dataset, so only process image
    if test_data:
        print("Creating test data batches...")
        data = tf.data.Dataset.from_tensor_slices(tf.constant(x))
        data_batch = data.map(process_image).batch(batch_size)
    # We don't need to shuffle the dataset if we have a validation dataset
    elif val_data:
        print("Creating validation data batches...")
        data = tf.data.Dataset.from_tensor_slices((tf.constant(x),
                                                  tf.constant(y)))
        data_batch = data.map(get_image_label).batch(batch_size)
     # With training data, we'd like to shuffle it before training
    else:
        print("Creating training data batches...")
        data = tf.data.Dataset.from_tensor_slices((tf.constant(x),
                                                   tf.constant(y)))
        # Shuffle first before image processing
        data = data.shuffle(buffer_size=len(x))

        # Get image,label tuples
        data = data.map(get_image_label)
        # Get data batches
        data_batch = data.batch(batch_size)
    
    return data_batch

# Create training and validation data batches 
train_data = create_data_batches(X_train, y_train)
val_data = create_data_batches(X_val,y_val)

train_data.element_spec, val_data.element_spec

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define the transformations you want to apply
data_generator = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Generate augmented images from a directory of images
train_generator = data_generator.flow_from_directory(
    '/content/drive/MyDrive/PlantSeedlings/train',
    target_size=(224, 224),
    batch_size=32,
    class_mode='categorical',
    shuffle=True
)

"""Now, we've our data batches in the form of tensors with the correct form. Before moving to the training, let's visualize what is going on with the functions above to better understand what we're working on. 
### Visualizing data batches
"""

# Create a function for viewing images in the data batch

def show_10_images(images, labels):
    # Setup the figure
    plt.figure(figsize=(10, 10))

    for i in range(10):
        # Create subplots (5 rows, 5 columns)
        ax = plt.subplot(5, 5, i+1)
        # Display an image
        plt.imshow(images[i])
        # Add the image label as the title
        plt.title(unique_species[labels[i].argmax()])
        # Turn gird lines off
        plt.axis("off")

"""We batchified the datasets, but we can't visualize data bathces directly. First, we need to un-do it for visualizing."""

train_images, train_labels = next(train_data.as_numpy_iterator())
show_10_images(train_images, train_labels)

"""This looks excellent! Each time you run the cell above you'll end up with a different code because create_data_batch function shuffles the data. 

Let's visualize the validation set.
"""

val_images, val_labels = next(val_data.as_numpy_iterator())
show_10_images(val_images, val_labels)

"""### Data augmentation for a better performance

### Creating and training a model

Since we have our data ready for deep learning algoritms to train, let's build a deep learning model. We'll use a pre-trained model for this task. This called "transfer learning". There are pre-trained deep learning you can find on TensorFlow website. 

### Building a model
Things that we need to define:
* The input shape.(Images in the form of Tensors)
* The output shape. (image labels, in the form of Tensors)
* The URL of the model we want to use.

### Building our own evaluation metric

The evaluation metric for our dataset is micro-averaged f1 score. It is not a built-in evaluation metric and we need to create it first.
"""

# The evaluation metric we'll use
import tensorflow as tf

def f1_micro(y_true, y_pred):
    y_true = tf.cast(y_true, tf.float32)
    y_pred = tf.round(tf.clip_by_value(y_pred, 0, 1))

    tp = tf.reduce_sum(y_true * y_pred)
    fp = tf.reduce_sum((1 - y_true) * y_pred)
    fn = tf.reduce_sum(y_true * (1 - y_pred))

    precision = tp / (tp + fp + tf.keras.backend.epsilon())
    recall = tp / (tp + fn + tf.keras.backend.epsilon())

    f_score = 2 * precision * recall / (precision + recall + tf.keras.backend.epsilon())

    return f_score

import tensorflow as tf
import tensorflow_hub as hub

# Setup the input shape 
INPUT_SHAPE = [None, IMG_SIZE, IMG_SIZE, 3]

# Setup the output shape
OUTPUT_SHAPE = (len(unique_species))

# Setup model URL from TensorFlow Hub
MODEL_URL = "https://tfhub.dev/google/imagenet/mobilenet_v2_130_224/classification/5"

# Create a function that builds a Keras model
def create_model(input_shape=INPUT_SHAPE, output_shape=OUTPUT_SHAPE, model_url=MODEL_URL):
    print(f"Building a model with {MODEL_URL}...")

    # Setup model layers
    model = tf.keras.Sequential([
        hub.KerasLayer(MODEL_URL), # Layer 1 (input layer)
        tf.keras.layers.Dense(units=OUTPUT_SHAPE,
                              activation="softmax") # The output layer
    ])
    # Compile the model
    model.compile(
        loss=tf.keras.losses.CategoricalCrossentropy(),
        optimizer=tf.keras.optimizers.Adam(), # For improvement
        metrics=["accuracy", f1_micro])
    # Build the model
    model.build(INPUT_SHAPE) # Let the model know the output it'll be getting

    return model

model = create_model()
model.summary()

"""### TensorBoard Callback

The callbacks are necessary to keep track of the model's progress. They have two functions: To imform us the time model started training and to halt model's training if it stop improving its performance over defined consecutive trials. 

To create the callback and view it in TensorBoard notebook we need to three things:
1. Load the TensorBoard notebook extension. 
2. Create a TensorBoard callback which is able to save logs to a directory and pass it to our model's `fit()` function. 
3. Visualize our model's training logs using `%tensorboard` magic function.
"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension 
# %load_ext tensorboard

import datetime

# Create a function to build a TensorBoard callback
def create_tensorboard_callback():
    # Create a log directory for storing TensorBoard logs
    logdir = os.path.join("/content/drive/MyDrive/PlantSeedlings/logs",
                          # Make it so the logs get tracked whenever we run an experiment
                          datetime.datetime.now().strftime("%Y%m%d-%H%M%S"))
    return tf.keras.callbacks.TensorBoard(logdir)

# Early stopping callback
early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_f1_micro',
                                                  patience=5)

"""### Training the model on subset of data

As a final move, we need to define number of epochs. Epochs refer to a training where the model trains over the entire dataset. By increasing it, our model will have many exposures to the data. 
"""

# Check if GPU is available
print("GPU", "available!" if tf.config.list_physical_devices("GPU") else "not available.")

# Define epochs 
NUM_EPOCHS = 100

history = model.fit_generator(
    train_generator,
    epochs=10,
    validation_data=val_data,
    callbacks=[create_tensorboard_callback, early_stopping]
)

"""Let's create a function for training the model. The function will do the following:
* Create a model using `create_model()`
* Setup a TensorBoard callback using `create_tensorboard_callback()`
* Call the `fit()` function on our model to pass it over the training and validation data, determine the number of epochs and setting up the callbacks we previously created. 
* Return the fitted model.
"""

# Build a function to train and return a trained model
def train_model():

    # Create the model
    model = create_model()

    # Create new TensorBoard session everytime we train a model
    tensorboard = create_tensorboard_callback()



    # Fit the model to the data and passing it callbacks we created
    model.fit(x=train_data,
              epochs=NUM_EPOCHS,
              validation_data=val_data,
              validation_freq=1,  # Checks validation metrics every epoch
              callbacks=[tensorboard, early_stopping])
    return model

model = train_model()

model.evaluate(val_data)

from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=30,
        width_shift_range=0.1,
        height_shift_range=0.1,
        shear_range=0.2,
        zoom_range=0.2,
        horizontal_flip=True,
        fill_mode='nearest')

train_generator = train_datagen.flow_from_directory(
        '/content/drive/MyDrive/PlantSeedlings/train',
        target_size=(IMG_SIZE, IMG_SIZE),
        batch_size=BATCH_SIZE,
        class_mode='categorical')
history = model.fit_generator(
        train_generator,
        steps_per_epoch=len(train_generator),
        epochs=50,
        validation_data=val_generator,
        validation_steps=len(val_generator))

"""### Making and evaluating predictions with the trained model"""

# Make predictions on the validation data (not used to train on)
predictions = model.predict(val_data, verbose=1) # verbose shows us how long there is to go
predictions

# Check the shape of predictions
predictions.shape

# First prediction
print(predictions[0])
print(f"Max value (probability of prediction): {np.max(predictions[0])}")
print(f"Sum: {np.sum(predictions[0])}") # because we used softmax activation in our model, this will be close to 1
print(f"Max index: {np.argmax(predictions[0])}") # the index of where the max value in predictions[0] occurs
print(f"Predicted label: {unique_species[np.argmax(predictions[0])]}") # the predicted label

# Let's write a function to turn prediction probabilities into their respected label
def get_pred_label(pred_probs):
    return unique_species[np.argmax(pred_probs)]

pred_label = get_pred_label(predictions[0])
pred_label

"""Since the model still hasn't trained on the validation data, we can make prediction on the validation data and compare them to actual labels and visually plot them.

However, our validation data is in the form of (images,labels) and in batches of 32. First, we need to unbatch it.
"""

def unbatchify(data):
    images = []
    labels = []

    # Loop through the unbatched data
    for image, label in data.unbatch().as_numpy_iterator():
        images.append(image)
        labels.append(unique_species[np.argmax(label)])
    return images, labels

# Unbatchify the validation data
val_images, val_labels = unbatchify(val_data)
val_images[0], val_labels[0]

def plot_pred(prediction_probabilities, labels, images, n=1):
  """
  View the prediction, ground truth label and image for sample n.
  """
  pred_prob, true_label, image = prediction_probabilities[n], labels[n], images[n]
  
  # Get the pred label
  pred_label = get_pred_label(pred_prob)
  
  # Plot image & remove ticks
  plt.imshow(image)
  plt.xticks([])
  plt.yticks([])

  # Change the color of the title depending on if the prediction is right or wrong
  if pred_label == true_label:
    color = "green"
  else:
    color = "red"

  plt.title("{} {:2.0f}% ({})".format(pred_label,
                                      np.max(pred_prob)*100,
                                      true_label),
                                      color=color)

# View an example prediction, original image and truth label
plot_pred(prediction_probabilities=predictions,
          labels=val_labels,
          images=val_images,
          n=156)

def plot_pred_conf(prediction_probabilities, labels, n=1):
  """
  Plots the top 10 highest prediction confidences along with
  the truth label for sample n.
  """
  pred_prob, true_label = prediction_probabilities[n], labels[n]

  # Get the predicted label
  pred_label = get_pred_label(pred_prob)

  # Find the top 10 prediction confidence indexes
  top_10_pred_indexes = pred_prob.argsort()[-10:][::-1]
  # Find the top 10 prediction confidence values
  top_10_pred_values = pred_prob[top_10_pred_indexes]
  # Find the top 10 prediction labels
  top_10_pred_labels = unique_species[top_10_pred_indexes]

  # Setup plot
  top_plot = plt.bar(np.arange(len(top_10_pred_labels)), 
                     top_10_pred_values, 
                     color="grey")
  plt.xticks(np.arange(len(top_10_pred_labels)),
             labels=top_10_pred_labels,
             rotation="vertical")

  # Change color of true label
  if np.isin(true_label, top_10_pred_labels):
    top_plot[np.argmax(top_10_pred_labels == true_label)].set_color("green")
  else:
    pass

# Let's check a few predictions and their different values
i_multiplier = 0
num_rows = 3
num_cols = 2
num_images = num_rows*num_cols
plt.figure(figsize=(5*2*num_cols, 5*num_rows))
for i in range(num_images):
  plt.subplot(num_rows, 2*num_cols, 2*i+1)
  plot_pred(prediction_probabilities=predictions,
            labels=val_labels,
            images=val_images,
            n=i+i_multiplier)
  plt.subplot(num_rows, 2*num_cols, 2*i+2)
  plot_pred_conf(prediction_probabilities=predictions,
                labels=val_labels,
                n=i+i_multiplier)
plt.tight_layout(h_pad=1.0)
plt.show()

# Evaluate the pre-saved model
model.evaluate(val_data)